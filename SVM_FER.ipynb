{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-528c8f984f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import errno\n",
    "import scipy.misc\n",
    "import dlib\n",
    "import cv2\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"preparing\")\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "original_labels = [0, 1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(image, rects):\n",
    "    # this function have been copied from http://bit.ly/2cj7Fpq\n",
    "    if len(rects) > 1:\n",
    "        raise BaseException(\"TooManyFaces\")\n",
    "    if len(rects) == 0:\n",
    "        raise BaseException(\"NoFaces\")\n",
    "    return np.matrix([[p.x, p.y] for p in predictor(image, rects[0]).parts()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_LABELS = [0,1,2,3,4,5,6]\n",
    "new_labels = list(set(original_labels) & set(SELECTED_LABELS))\n",
    "nb_images_per_label = list(np.zeros(len(new_labels), 'uint8'))\n",
    "def get_new_label(label, one_hot_encoding=False):\n",
    "    if one_hot_encoding:\n",
    "        new_label = new_labels.index(label)\n",
    "        label = list(np.zeros(len(new_labels), 'uint8'))\n",
    "        label[new_label] = 1\n",
    "        return label\n",
    "    else:\n",
    "        return new_labels.index(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"importing csv file\")\n",
    "data = pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Usage'].unique())\n",
    "image_height = 48\n",
    "image_width = 48\n",
    "window_size = 24\n",
    "window_step = 6\n",
    "\n",
    "for category in data['Usage'].unique():\n",
    "    print( \"converting set: \" + category + \"...\")\n",
    "    if not os.path.exists(category):\n",
    "        try:\n",
    "            os.makedirs(category)\n",
    "        except OSError as e:\n",
    "            print(\"error!: \", e)\n",
    "    \n",
    "    # get samples and labels of the actual category\n",
    "    category_data = data[data['Usage'] == category]\n",
    "    samples = category_data['pixels'].values\n",
    "    labels = category_data['emotion'].values\n",
    "    \n",
    "    # get images and extract features\n",
    "    images = []\n",
    "    labels_list = []\n",
    "    landmarks = []\n",
    "    hog_features = []\n",
    "    hog_images = []\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        try:\n",
    "            image = np.fromstring(samples[i], dtype=int, sep=\" \").reshape((image_height, image_width))\n",
    "            images.append(image)\n",
    "            scipy.misc.imsave('temp.jpg', image)\n",
    "            image2 = cv2.imread('temp.jpg')\n",
    "            \n",
    "            '''Extracting face landmarks here'''\n",
    "            face_rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]\n",
    "            face_landmarks = get_landmarks(image2, face_rects)\n",
    "            landmarks.append(face_landmarks) \n",
    "            labels_list.append(get_new_label(labels[i], one_hot_encoding=True))\n",
    "            nb_images_per_label[get_new_label(labels[i])] += 1\n",
    "            \n",
    "            features, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                                            cells_per_block=(1, 1), visualise=True)\n",
    "            hog_features.append(features)\n",
    "            hog_images.append(hog_image)\n",
    "        except Exception as e:\n",
    "            print( \"error in image: \" + str(i) + \" - \" + str(e))\n",
    "    \n",
    "    np.save(\"./\"+category+'/images.npy', images)\n",
    "    np.save(\"./\"+category+'/landmarks.npy', landmarks)\n",
    "    np.save(\"./\"+category+'/labels.npy', labels_list)\n",
    "    np.save(\"./\"+category+'/hog_features.npy', hog_features)\n",
    "    np.save(\"./\"+category+'/hog_images.npy', hog_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "validation_dict = dict()\n",
    "test_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['X'] = np.load('./Training' + '/landmarks.npy')\n",
    "data_dict['X'] = np.array([x.flatten() for x in data_dict['X']])\n",
    "data_dict['X'] = np.concatenate((data_dict['X'], np.load('./Training' + '/hog_features.npy')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Y'] = np.load('./Training' +  '/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dict['X'] = np.load('./PublicTest' + '/landmarks.npy')\n",
    "validation_dict['X'] = np.array([x.flatten() for x in validation_dict['X']])\n",
    "validation_dict['Y'] = np.load('./PublicTest'  + '/labels.npy')\n",
    "validation_dict['X'] = np.concatenate((validation_dict['X'], np.load('./PublicTest' + '/hog_features.npy')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['X'] = np.load('./PrivateTest' + '/landmarks.npy')\n",
    "test_dict['X'] = np.array([x.flatten() for x in test_dict['X']])\n",
    "test_dict['Y'] = np.load('./PrivateTest'  + '/labels.npy')\n",
    "test_dict['X'] = np.concatenate((test_dict['X'], np.load('./PrivateTest' + '/hog_features.npy')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"building model...\")\n",
    "#model = SVC(random_state=42, max_iter=300)\n",
    "model = SVC(random_state=42, max_iter=10000, kernel='linear', gamma=0.0001, decision_function_shape = 'ovr')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "start_time = time.time()\n",
    "dict_y=np.ndarray(len(data_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(data_dict['Y'])):\n",
    "    temp = data_dict['Y'][row][0]\n",
    "    dict_y[count] = temp\n",
    "    count+=1\n",
    "model.fit(data_dict['X'], dict_y)\n",
    "training_time = time.time() - start_time\n",
    "print( \"training time = {0:.1f} sec\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y):\n",
    "    predicted_Y = model.predict(X)\n",
    "    accuracy = accuracy_score(Y, predicted_Y)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_v=np.ndarray(len(validation_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(validation_dict['Y'])):\n",
    "    temp = validation_dict['Y'][row][0]\n",
    "    dict_v[count] = temp\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = evaluate(model, validation_dict['X'], dict_v)\n",
    "print( \"  - validation accuracy = {0:.1f}\".format(validation_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_t=np.ndarray(len(test_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(test_dict['Y'])):\n",
    "    temp = test_dict['Y'][row][0]\n",
    "    dict_t[count] = temp\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(model, test_dict['X'], dict_t)\n",
    "print( \"  - test accuracy = {0:.1f}\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using ONLY HOG features'''\n",
    "data_hog_train = np.load('./Training' + '/hog_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(data_hog_train, dict_y)\n",
    "training_time = time.time() - start_time\n",
    "print( \"training time = {0:.1f} sec\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_validation_hog = np.load('./PublicTest' + '/hog_features.npy')\n",
    "validation_accuracy = evaluate(model, data_validation_hog, dict_v)\n",
    "print( \"  - validation accuracy = {0:.1f}\".format(validation_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_hog = np.load('./PrivateTest' + '/hog_features.npy')\n",
    "test_accuracy = evaluate(model, data_test_hog, dict_t)\n",
    "print( \"  - test accuracy = {0:.1f}\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding Training + Valid '''\n",
    "data_dict['X'] = np.load('./Training' + '/landmarks.npy')\n",
    "data_dict['X'] = np.array([x.flatten() for x in data_dict['X']])\n",
    "validation_dict['X'] = np.load('./PublicTest' + '/landmarks.npy')\n",
    "validation_dict['X'] = np.array([x.flatten() for x in validation_dict['X']])\n",
    "data_dict['X'] = np.concatenate((data_dict['X'], validation_dict['X']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y = np.concatenate((dict_y, dict_v), axis = 0)\n",
    "model.fit(data_dict['X'], dict_y)\n",
    "training_time = time.time() - start_time\n",
    "print( \"training time = {0:.1f} sec\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['X'] = np.load('./PrivateTest' + '/landmarks.npy')\n",
    "test_dict['X'] = np.array([x.flatten() for x in test_dict['X']])\n",
    "test_accuracy = evaluate(model, test_dict['X'], dict_t)\n",
    "print( \"  - test accuracy = {0:.1f}\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Only Face Landmarks'''\n",
    "\n",
    "data_dict['X'] = np.load('./Training' + '/landmarks.npy')\n",
    "data_dict['X'] = np.array([x.flatten() for x in data_dict['X']])\n",
    "\n",
    "validation_dict['X'] = np.load('./PublicTest' + '/landmarks.npy')\n",
    "validation_dict['X'] = np.array([x.flatten() for x in validation_dict['X']])\n",
    "\n",
    "test_dict['X'] = np.load('./PrivateTest' + '/landmarks.npy')\n",
    "test_dict['X'] = np.array([x.flatten() for x in test_dict['X']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=42, max_iter=10000, kernel='linear', gamma=0.0001, decision_function_shape = 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Y'] = np.load('./Training' +  '/labels.npy')\n",
    "dict_y = np.ndarray(len(data_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(data_dict['Y'])):\n",
    "    temp = data_dict['Y'][row][0]\n",
    "    dict_y[count] = temp\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.fit(data_dict['X'], dict_y)\n",
    "training_time = time.time() - start_time\n",
    "print( \"training time = {0:.1f} sec\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['Y'] = np.load('./PrivateTest'  + '/labels.npy')\n",
    "dict_t=np.ndarray(len(test_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(test_dict['Y'])):\n",
    "    temp = test_dict['Y'][row][0]\n",
    "    dict_t[count] = temp\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(model, test_dict['X'], dict_t)\n",
    "print(\"  - test accuracy = {0:.1f}%\".format(test_accuracy*100))\n",
    "print(\"  - training accuracy:  = {0:.1f}%\".format(model.score(data_dict['X'], dict_y)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dict['Y'] = np.load('./PrivateTest'  + '/labels.npy')\n",
    "dict_t=np.ndarray(len(validation_dict['Y']))\n",
    "count=0\n",
    "for row in range(len(validation_dict['Y'])):\n",
    "    temp = validation_dict['Y'][row][0]\n",
    "    dict_t[count] = temp\n",
    "    count+=1\n",
    "validation_accuracy = evaluate(model, validation_dict['X'], dict_v)\n",
    "print(\"  - validation accuracy = {0:.1f}%\".format(validation_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
